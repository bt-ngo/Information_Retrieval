{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ir-evaluation-py in c:\\users\\bap_bap\\anaconda3\\lib\\site-packages (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install ir-evaluation-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bap_Bap\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\Bap_Bap\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.QVLO2T66WEPI7JZ63PS3HMOHFEY472BC.gfortran-win_amd64.dll\n",
      "C:\\Users\\Bap_Bap\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "import operator\n",
    "from ir_evaluation.effectiveness import effectiveness\n",
    "import time\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_sets(obj):\n",
    "    if isinstance(obj, set):\n",
    "        return list(obj)\n",
    "\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(content):\n",
    "    stop_list = stopwords.words('english')\n",
    "    stem = PorterStemmer()\n",
    "\n",
    "    content = content.lstrip()\n",
    "    content = re.sub('[^A-Za-z]+', ' ', content)\n",
    "    tokens = nltk.word_tokenize(content)\n",
    "  \n",
    "    stem_tokens = [stem.stem(word) for word in tokens if word not in stop_list]\n",
    "    clean_stem_tokens = ' '.join(map(str,  stem_tokens))\n",
    "\n",
    "    shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "    clean_stem_tokens = shortword.sub('', clean_stem_tokens)\n",
    "\n",
    "    return clean_stem_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create two folders to stores files of Cranfield dataset (cran/) and NFCorpus test (nfcorpus/)\n",
    "For each dataset, create folders to store processed data:\n",
    "- docs_files: a folder to store each raw doc in separated file\n",
    "- processed_docs: a folder to store processed doc in separated file\n",
    "- query_files: a folder to store each raw query in separated file\n",
    "- vsm_result: a folder to store ranked list of VSM model of each doc in separated file\n",
    "- prob_result: a folder to store ranked list of VSM model of each doc in separated file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Cranfield 1400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store each doc in a seperated file\n",
    "with open('./corpus/cran/cran.all.1400', 'r') as f:\n",
    "    content = f.readlines()\n",
    "    for line in content:\n",
    "      if line.startswith('.I'):\n",
    "          f_ = line.split('.I')[1]\n",
    "          name = './cran/doc_files/' + f_[:-1].strip() + '.txt'\n",
    "          f1 = open(name, 'w')\n",
    "          f1.write(line)\n",
    "          f1.close()\n",
    "      else:\n",
    "          with open(name, 'a') as f1:\n",
    "              f1.write(line)\n",
    "          f1.close()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save processed doc\n",
    "dir = os.listdir('./cran/doc_files/')\n",
    "for d in dir:\n",
    "    with open('./cran/doc_files/' + d, 'r') as f:\n",
    "        content = f.readlines()\n",
    "        flag = False\n",
    "        content_ = ''\n",
    "        for line in content:\n",
    "            if line.startswith('.W'):\n",
    "                flag = True\n",
    "            if flag == True:\n",
    "                if line.startswith('.W'):\n",
    "                    continue\n",
    "                else:\n",
    "                    content_ = content_ + line\n",
    "        new_content = text_process(content_)\n",
    "        with open('./cran/processed_doc/' + d, 'w') as f1:\n",
    "            f1.write(new_content)\n",
    "            f1.close()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. NFCorpus-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store each doc in a seperated file\n",
    "with open('./corpus/nfcorpus/test.docs', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        id, doc = line.split('\\t')\n",
    "        f1 = open('./nfcorpus/doc_files/' + id + '.txt', 'w')\n",
    "        f1.write(doc)\n",
    "        f1.close()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save processed doc\n",
    "dir = os.listdir('./nfcorpus/doc_files/')\n",
    "for d in dir:\n",
    "    with open('./nfcorpus/doc_files/' + d, 'r') as f:\n",
    "        content = f.read()\n",
    "        new_content = text_process(content)\n",
    "        with open('./nfcorpus/processed_doc/' + d, 'w') as f1:\n",
    "            f1.write(new_content)\n",
    "            f1.close()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_index(path):\n",
    "    inverted_index = {} \n",
    "    dir = os.listdir(path)\n",
    "    no_docs = len(dir)\n",
    "    for d in dir:\n",
    "        id = d.split('.')[0]\n",
    "        f = open(path + d, 'r')\n",
    "        content = f.read()\n",
    "        f.close()\n",
    "        tokens = nltk.word_tokenize(content)\n",
    "        for token in tokens:\n",
    "            if token not in inverted_index:\n",
    "                inverted_index[token] = {}\n",
    "            if id not in inverted_index[token]:\n",
    "                inverted_index[token][id]= 1\n",
    "            else:\n",
    "                inverted_index[token][id] += 1\n",
    "    doc = {}\n",
    "    for term in inverted_index:\n",
    "        idf = np.log10(no_docs/len(inverted_index[term]))\n",
    "        for id in inverted_index[term]:\n",
    "            if id not in doc:\n",
    "                doc[id] = []\n",
    "            inverted_index[term][id] = (1 + np.log10(inverted_index[term][id]))*idf\n",
    "            doc[id].append(inverted_index[term][id])\n",
    "    doc_norm = {}\n",
    "    for id in doc:\n",
    "        sqr = np.square(doc[id])\n",
    "        sum_sqr = np.sum(sqr)\n",
    "        norm = np.sqrt(sum_sqr)\n",
    "        doc_norm[id] = norm\n",
    "    for term in inverted_index:\n",
    "        for id in inverted_index[term]:\n",
    "            inverted_index[term][id] = inverted_index[term][id]/doc_norm[id]\n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prob_index(path):\n",
    "    inverted_index = {} \n",
    "    doc_len = {}\n",
    "    dir = os.listdir(path)\n",
    "    L_c = 0\n",
    "    for d in dir:\n",
    "        id = d.split('.')[0]\n",
    "        f = open(path + d, 'r')\n",
    "        content = f.read()\n",
    "        f.close()\n",
    "        tokens = nltk.word_tokenize(content)\n",
    "        doc_length = len(tokens)\n",
    "        doc_len[id] = doc_length\n",
    "        L_c += doc_length\n",
    "        for token in tokens:\n",
    "            if token not in inverted_index:\n",
    "                inverted_index[token] = {}\n",
    "                inverted_index[token]['all'] = 0\n",
    "            if id not in inverted_index[token]:\n",
    "                inverted_index[token][id] = 1\n",
    "            else: inverted_index[token][id] += 1\n",
    "            inverted_index[token]['all'] += 1\n",
    "    for term in inverted_index:\n",
    "        inverted_index[term]['all'] =  inverted_index[term]['all']/L_c\n",
    "        for id in inverted_index[term]:\n",
    "            if id == 'all':\n",
    "                continue\n",
    "            inverted_index[term][id] = inverted_index[term][id]/doc_len[id]\n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Cranfield 1400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.255183219909668\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "index = create_vector_index('./cran/processed_doc/')\n",
    "f = open('./cran/inverted_index_vect.txt', 'w')\n",
    "f.write(json.dumps(index))\n",
    "f.close()\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3352742195129395\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "index= create_prob_index('./cran/processed_doc/')\n",
    "f = open('./cran/inverted_index_prob.txt', 'w')\n",
    "f.write(json.dumps(index))\n",
    "f.close()\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. NFCorpus-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.070080757141113\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "index = create_vector_index('./nfcorpus/processed_doc/')\n",
    "f = open('./nfcorpus/inverted_index_vect.txt', 'w')\n",
    "f.write(json.dumps(index))\n",
    "f.close()\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.028642654418945\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "index = create_prob_index('./nfcorpus/processed_doc/')\n",
    "f = open('./nfcorpus/inverted_index_prob.txt', 'w')\n",
    "f.write(json.dumps(index))\n",
    "f.close()\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(content):\n",
    "    stop_list = stopwords.words('english')\n",
    "    stem = PorterStemmer()\n",
    "\n",
    "    content = content.lstrip()\n",
    "    content = re.sub('[^A-Za-z]+', ' ', content)\n",
    "    tokens = nltk.word_tokenize(content)\n",
    "  \n",
    "    stem_tokens = [stem.stem(word) for word in tokens if word not in stop_list]\n",
    "    clean_stem_tokens = ' '.join(map(str,  stem_tokens))\n",
    "\n",
    "    shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "    clean_stem_tokens = shortword.sub('', clean_stem_tokens)\n",
    "\n",
    "    return clean_stem_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(qry):\n",
    "    return text_process(qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_index(index_path):\n",
    "    f = open(index_path, 'r')\n",
    "    data = f.read()\n",
    "    index = json.loads(data)\n",
    "    f.close()\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vsm_retrieve(qry, index):\n",
    "    rst = {}\n",
    "    tokens = nltk.word_tokenize(qry)\n",
    "    for token in tokens:\n",
    "        if token not in index:\n",
    "            continue\n",
    "        for doc_id in index[token]:\n",
    "            if doc_id not in rst:\n",
    "                rst[doc_id] = index[token][doc_id]\n",
    "            else:\n",
    "                rst[doc_id] += index[token][doc_id]\n",
    "    ranked_list = dict(sorted(rst.items(), key=operator.itemgetter(1),reverse=True))\n",
    "    return list(ranked_list.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_likelihood_retrieve(qry, index, l=0.5):\n",
    "    rst = {}\n",
    "    tokens = nltk.word_tokenize(qry)\n",
    "    token_in = []\n",
    "    for token in tokens:\n",
    "        if token not in index:\n",
    "            continue\n",
    "        token_in.append(token)\n",
    "        for doc_id in index[token]:\n",
    "            if doc_id == 'all':\n",
    "                continue\n",
    "            rst[doc_id] = 0\n",
    "    for id in rst:\n",
    "        for token in token_in:\n",
    "            if index[token].get(id) is None:\n",
    "                rst[id] += np.log10((1-l)*index[token]['all'])\n",
    "            else: rst[id] += np.log10(l*index[token][id] + (1-l)*index[token]['all'])\n",
    "    ranked_list = dict(sorted(rst.items(), key=operator.itemgetter(1),reverse=True))\n",
    "    return list(ranked_list.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Cranfield 1400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save each query in seperate file\n",
    "f = open('./corpus/cran/cran.qry', 'r')\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "flag = False\n",
    "count = 1\n",
    "for line in lines:\n",
    "    if line.startswith('.I'):\n",
    "        if flag == True:\n",
    "            f = open('./cran/query_files/' + qry_id + '.txt', 'w')\n",
    "            f.write(qry)\n",
    "            f.close()\n",
    "        qry_id = str(count)\n",
    "        qry = ''\n",
    "        count += 1\n",
    "        flag = False\n",
    "    elif line.startswith('.W'):\n",
    "        flag = True\n",
    "    else: qry += line\n",
    "f = open('./cran/query_files/' + qry_id + '.txt', 'w')\n",
    "f.write(qry)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5505785942077637\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "index = read_index('./cran/inverted_index_vect.txt')\n",
    "query_list = os.listdir('./cran/query_files/')\n",
    "for name in query_list:\n",
    "    qry_id = name.split('.')[0]\n",
    "    f = open('./cran/query_files/' + name, 'r')\n",
    "    qry = f.read()\n",
    "    f.close()\n",
    "    qry = process_query(qry)\n",
    "    retrieval_result = vsm_retrieve(qry, index)\n",
    "    f = open('./cran/vsm_result/' + name, 'w')\n",
    "    f.write(json.dumps(retrieval_result))\n",
    "    f.close()\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.739270448684692\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "index = read_index('./cran/inverted_index_prob.txt')\n",
    "query_list = os.listdir('./cran/query_files/')\n",
    "for name in query_list:\n",
    "    qry_id = name.split('.')[0]\n",
    "    f = open('./cran/query_files/' + name, 'r')\n",
    "    qry = f.read()\n",
    "    f.close()\n",
    "    qry = process_query(qry)\n",
    "    retrieval_result = query_likelihood_retrieve(qry, index, l=0.5)\n",
    "    f = open('./cran/prob_result/' + name, 'w')\n",
    "    f.write(json.dumps(retrieval_result))\n",
    "    f.close()\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. NFCorpus-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./corpus/nfcorpus/test.all.queries', 'r', encoding='utf-8')\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "for line in lines:\n",
    "    if line == '':\n",
    "        continue\n",
    "    qry_id, qry = line.split('\\t')\n",
    "    f = open('./nfcorpus/query_files/' + qry_id + '.txt', 'w', encoding='utf-8')\n",
    "    f.write(qry)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.61549711227417\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "index = read_index('./nfcorpus/inverted_index_vect.txt')\n",
    "query_list = os.listdir('./nfcorpus/query_files/')\n",
    "for name in query_list:\n",
    "    qry_id = name.split('.')[0]\n",
    "    f = open('./nfcorpus/query_files/' + name, 'r', encoding='utf-8')\n",
    "    qry = f.read()\n",
    "    f.close()\n",
    "    qry = process_query(qry)\n",
    "    retrieval_result = vsm_retrieve(qry, index)\n",
    "    f = open('./nfcorpus/vsm_result/' + name, 'w', encoding='utf-8')\n",
    "    f.write(json.dumps(retrieval_result))\n",
    "    f.close()\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1735.146465063095\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "index = read_index('./nfcorpus/inverted_index_prob.txt')\n",
    "query_list = os.listdir('./nfcorpus/query_files/')\n",
    "for name in query_list:\n",
    "    qry_id = name.split('.')[0]\n",
    "    f = open('./nfcorpus/query_files/' + name, 'r', encoding='utf-8')\n",
    "    qry = f.read()\n",
    "    f.close()\n",
    "    qry = process_query(qry)\n",
    "    retrieval_result = query_likelihood_retrieve(qry, index, l=0.5)\n",
    "    f = open('./nfcorpus/prob_result/' + name, 'w', encoding='utf-8')\n",
    "    f.write(json.dumps(retrieval_result))\n",
    "    f.close()\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --> an object, which we can use all methods in it, is created\n",
    "ir = effectiveness()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Cranfield 1400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mô hình không gian vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "ir = effectiveness()\n",
    "interactions = {}\n",
    "\n",
    "f = open('./corpus/cran/cranqrel', 'r')\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "for line in lines:\n",
    "    qry_id, doc_id, _ = line.split()\n",
    "    if interactions.get(qry_id) is None:\n",
    "        interactions[qry_id] = {}\n",
    "\n",
    "    if interactions[qry_id].get('related_documents') is None:\n",
    "        interactions[qry_id]['related_documents'] = set()\n",
    "    interactions[qry_id]['related_documents'].add(doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = os.listdir('./cran/vsm_result/')\n",
    "for d in dir:\n",
    "    qry_id = d.split('.')[0]\n",
    "    f = open('./cran/vsm_result/' + d, 'r')\n",
    "    content = f.read()\n",
    "    f.close()\n",
    "    rst_list = json.loads(content)\n",
    "    interactions[qry_id]['total_result'] = len(rst_list)\n",
    "    for i in range(len(rst_list)):\n",
    "        if rst_list[i] in interactions[qry_id]['related_documents']:\n",
    "            if interactions[qry_id].get('visited_documents') is None:\n",
    "                interactions[qry_id]['visited_documents'] = []\n",
    "            interactions[qry_id]['visited_documents'].append(rst_list[i])\n",
    "            if interactions[qry_id].get('visited_documents_orders') is None:\n",
    "                interactions[qry_id]['visited_documents_orders'] = {}\n",
    "            interactions[qry_id]['visited_documents_orders'][rst_list[i]] = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "for qry_id in interactions:\n",
    "    if interactions[qry_id].get('total_result') is None:\n",
    "        interactions[qry_id]['total_result'] =  '0'\n",
    "    if interactions[qry_id].get('visited_documents') is None:\n",
    "        interactions[qry_id]['visited_documents'] = []\n",
    "    if interactions[qry_id].get('visited_documents_orders') is None:\n",
    "        interactions[qry_id]['visited_documents_orders'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision:\n",
      "{'all': {'count': 225, 'value': 0.38703378534464195}}\n"
     ]
    }
   ],
   "source": [
    "print (\"Mean Average Precision:\")\n",
    "mean_ap = ir.mean_ap(interactions,['all'])\n",
    "print(mean_ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eleven Point - Interpolated Average Precision:\n",
      "Recall => Precision\n",
      "0.0 : 0.7861359193438143\n",
      "0.1 : 0.7512610302073218\n",
      "0.2 : 0.6392701577396239\n",
      "0.3 : 0.5329488259116696\n",
      "0.4 : 0.4518462924363577\n",
      "0.5 : 0.38672406643723584\n",
      "0.6 : 0.2971547690926209\n",
      "0.7 : 0.2209523838865997\n",
      "0.8 : 0.18170204096079456\n",
      "0.9 : 0.13069001005036904\n",
      "1.0 : 0.11511882697965727\n"
     ]
    }
   ],
   "source": [
    "print (\"Eleven Point - Interpolated Average Precision:\")\n",
    "print (\"Recall => Precision\")\n",
    "iap = ir.iap(interactions)\n",
    "for key in iap:\n",
    "    print(key, ':', iap[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./cran/vsm_rst.txt', 'w')\n",
    "f.write(json.dumps(interactions, default=serialize_sets))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mô hình xác suất Query Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "ir = effectiveness()\n",
    "interactions = {}\n",
    "\n",
    "f = open('./corpus/cran/cranqrel', 'r')\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "for line in lines:\n",
    "    qry_id, doc_id, _ = line.split()\n",
    "    if interactions.get(qry_id) is None:\n",
    "        interactions[qry_id] = {}\n",
    "\n",
    "    if interactions[qry_id].get('related_documents') is None:\n",
    "        interactions[qry_id]['related_documents'] = set()\n",
    "    interactions[qry_id]['related_documents'].add(doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = os.listdir('./cran/prob_result/')\n",
    "for d in dir:\n",
    "    qry_id = d.split('.')[0]\n",
    "    f = open('./cran/prob_result/' + d, 'r')\n",
    "    content = f.read()\n",
    "    f.close()\n",
    "    rst_list = json.loads(content)\n",
    "    interactions[qry_id]['total_result'] = len(rst_list)\n",
    "    for i in range(len(rst_list)):\n",
    "        if rst_list[i] in interactions[qry_id]['related_documents']:\n",
    "            if interactions[qry_id].get('visited_documents') is None:\n",
    "                interactions[qry_id]['visited_documents'] = []\n",
    "            interactions[qry_id]['visited_documents'].append(rst_list[i])\n",
    "            if interactions[qry_id].get('visited_documents_orders') is None:\n",
    "                interactions[qry_id]['visited_documents_orders'] = {}\n",
    "            interactions[qry_id]['visited_documents_orders'][rst_list[i]] = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "for qry_id in interactions:\n",
    "    if interactions[qry_id].get('total_result') is None:\n",
    "        interactions[qry_id]['total_result'] =  '0'\n",
    "    if interactions[qry_id].get('visited_documents') is None:\n",
    "        interactions[qry_id]['visited_documents'] = []\n",
    "    if interactions[qry_id].get('visited_documents_orders') is None:\n",
    "        interactions[qry_id]['visited_documents_orders'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision:\n",
      "{'all': {'count': 225, 'value': 0.405603669730284}}\n"
     ]
    }
   ],
   "source": [
    "print (\"Mean Average Precision:\")\n",
    "mean_ap = ir.mean_ap(interactions,['all'])\n",
    "print(mean_ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eleven Point - Interpolated Average Precision:\n",
      "Recall => Precision\n",
      "0.0 : 0.8344000289198265\n",
      "0.1 : 0.8006503280034587\n",
      "0.2 : 0.6864603822848977\n",
      "0.3 : 0.554665330586629\n",
      "0.4 : 0.44945223825311714\n",
      "0.5 : 0.3887348060319673\n",
      "0.6 : 0.3113657273362951\n",
      "0.7 : 0.23169529902585412\n",
      "0.8 : 0.18918105618024614\n",
      "0.9 : 0.13492137570221535\n",
      "1.0 : 0.11857584570608407\n"
     ]
    }
   ],
   "source": [
    "print (\"Eleven Point - Interpolated Average Precision:\")\n",
    "print (\"Recall => Precision\")\n",
    "iap = ir.iap(interactions)\n",
    "for key in iap:\n",
    "    print(key, ':', iap[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./cran/prob_rst.txt', 'w')\n",
    "f.write(json.dumps(interactions, default=serialize_sets))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. NFCorpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mô hình không gian vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "ir = effectiveness()\n",
    "interactions = {}\n",
    "\n",
    "f = open('./corpus/nfcorpus/test.2-1-0.qrel', 'r')\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "for line in lines:\n",
    "    qry_id, _, doc_id, _ = line.split()\n",
    "    if interactions.get(qry_id) is None:\n",
    "        interactions[qry_id] = {}\n",
    "\n",
    "    if interactions[qry_id].get('related_documents') is None:\n",
    "        interactions[qry_id]['related_documents'] = set()\n",
    "    interactions[qry_id]['related_documents'].add(doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = os.listdir('./nfcorpus/vsm_result/')\n",
    "for d in dir:\n",
    "    qry_id = d.split('.')[0]\n",
    "    if qry_id not in interactions:\n",
    "        continue\n",
    "    f = open('./nfcorpus/vsm_result/' + d, 'r')\n",
    "    content = f.read()\n",
    "    f.close()\n",
    "    rst_list = json.loads(content)\n",
    "    interactions[qry_id]['total_result'] = len(rst_list)\n",
    "    for i in range(len(rst_list)):\n",
    "        if rst_list[i] in interactions[qry_id]['related_documents']:\n",
    "            if interactions[qry_id].get('visited_documents') is None:\n",
    "                interactions[qry_id]['visited_documents'] = []\n",
    "            interactions[qry_id]['visited_documents'].append(rst_list[i])\n",
    "            if interactions[qry_id].get('visited_documents_orders') is None:\n",
    "                interactions[qry_id]['visited_documents_orders'] = {}\n",
    "            interactions[qry_id]['visited_documents_orders'][rst_list[i]] = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "for qry_id in interactions:\n",
    "    if interactions[qry_id].get('total_result') is None:\n",
    "        interactions[qry_id]['total_result'] =  '0'\n",
    "    if interactions[qry_id].get('visited_documents') is None:\n",
    "        interactions[qry_id]['visited_documents'] = []\n",
    "    if interactions[qry_id].get('visited_documents_orders') is None:\n",
    "        interactions[qry_id]['visited_documents_orders'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision:\n",
      "{'all': {'count': 323, 'value': 0.2027836490060701}}\n"
     ]
    }
   ],
   "source": [
    "print (\"Mean Average Precision:\")\n",
    "mean_ap = ir.mean_ap(interactions,['all'])\n",
    "print(mean_ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eleven Point - Interpolated Average Precision:\n",
      "Recall => Precision\n",
      "0.0 : 0.6096476250282464\n",
      "0.1 : 0.43853398593582427\n",
      "0.2 : 0.334218016124052\n",
      "0.3 : 0.25722762149618733\n",
      "0.4 : 0.19817650741372284\n",
      "0.5 : 0.16648342869225694\n",
      "0.6 : 0.12595960971004824\n",
      "0.7 : 0.0916692814734618\n",
      "0.8 : 0.08233790962959209\n",
      "0.9 : 0.07083097659219478\n",
      "1.0 : 0.06272026337196357\n"
     ]
    }
   ],
   "source": [
    "print (\"Eleven Point - Interpolated Average Precision:\")\n",
    "print (\"Recall => Precision\")\n",
    "iap = ir.iap(interactions)\n",
    "for key in iap:\n",
    "    print(key, ':', iap[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./nfcorpus/vsm_rst.txt', 'w')\n",
    "f.write(json.dumps(interactions, default=serialize_sets))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mô hình Query Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "ir = effectiveness()\n",
    "interactions = {}\n",
    "\n",
    "f = open('./corpus/nfcorpus/test.2-1-0.qrel', 'r')\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "for line in lines:\n",
    "    qry_id, _, doc_id, _ = line.split()\n",
    "    if interactions.get(qry_id) is None:\n",
    "        interactions[qry_id] = {}\n",
    "\n",
    "    if interactions[qry_id].get('related_documents') is None:\n",
    "        interactions[qry_id]['related_documents'] = set()\n",
    "    interactions[qry_id]['related_documents'].add(doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = os.listdir('./nfcorpus/prob_result/')\n",
    "for d in dir:\n",
    "    qry_id = d.split('.')[0]\n",
    "    f = open('./nfcorpus/prob_result/' + d, 'r')\n",
    "    if qry_id not in interactions:\n",
    "        continue\n",
    "    content = f.read()\n",
    "    f.close()\n",
    "    rst_list = json.loads(content)\n",
    "    interactions[qry_id]['total_result'] = len(rst_list)\n",
    "    for i in range(len(rst_list)):\n",
    "        if rst_list[i] in interactions[qry_id]['related_documents']:\n",
    "            if interactions[qry_id].get('visited_documents') is None:\n",
    "                interactions[qry_id]['visited_documents'] = []\n",
    "            interactions[qry_id]['visited_documents'].append(rst_list[i])\n",
    "            if interactions[qry_id].get('visited_documents_orders') is None:\n",
    "                interactions[qry_id]['visited_documents_orders'] = {}\n",
    "            interactions[qry_id]['visited_documents_orders'][rst_list[i]] = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "for qry_id in interactions:\n",
    "    if interactions[qry_id].get('total_result') is None:\n",
    "        interactions[qry_id]['total_result'] =  '0'\n",
    "    if interactions[qry_id].get('visited_documents') is None:\n",
    "        interactions[qry_id]['visited_documents'] = []\n",
    "    if interactions[qry_id].get('visited_documents_orders') is None:\n",
    "        interactions[qry_id]['visited_documents_orders'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision:\n",
      "{'all': {'count': 323, 'value': 0.2172095682373519}}\n"
     ]
    }
   ],
   "source": [
    "print (\"Mean Average Precision:\")\n",
    "mean_ap = ir.mean_ap(interactions,['all'])\n",
    "print(mean_ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eleven Point - Interpolated Average Precision:\n",
      "Recall => Precision\n",
      "0.0 : 0.6421587821183629\n",
      "0.1 : 0.4648908812516916\n",
      "0.2 : 0.35977965517624294\n",
      "0.3 : 0.2773239377317193\n",
      "0.4 : 0.21195318144674277\n",
      "0.5 : 0.1822185724843052\n",
      "0.6 : 0.13751834797414308\n",
      "0.7 : 0.10037797675996199\n",
      "0.8 : 0.08913046005549542\n",
      "0.9 : 0.077174623744552\n",
      "1.0 : 0.06912051190070122\n"
     ]
    }
   ],
   "source": [
    "print (\"Eleven Point - Interpolated Average Precision:\")\n",
    "print (\"Recall => Precision\")\n",
    "iap = ir.iap(interactions)\n",
    "for key in iap:\n",
    "    print(key, ':', iap[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./nfcorpus/prob_rst.txt', 'w')\n",
    "f.write(json.dumps(interactions, default=serialize_sets))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ada9fabe7e6e53f306d2de459e56a9ae70b3c90146f086e5031f9817e1f8397e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
